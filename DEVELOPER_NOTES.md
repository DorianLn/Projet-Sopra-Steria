# üìù NOTES DE D√âVELOPPEMENT - Mistral Integration

## Architecture D√©cisions

### 1. Pas de d√©pendances Python suppl√©mentaires
**D√©cision:** Utiliser uniquement `urllib` de stdlib au lieu de `requests`
**Raison:** R√©duire les d√©pendances et la surface d'attaque
**Impact:** Code l√©g√®rement plus verbeux mais autonome

### 2. Classe + Fonction wrapper
**D√©cision:** Cr√©er une classe `MistralCVAnalyzer` ET une fonction `analyze_cv()`
**Raison:** Flexibilit√© - utilisateurs peuvent utiliser la classe ou la fonction
**Impact:** Deux APIs disponibles pour diff√©rents cas d'usage

### 3. Temp√©rature basse par d√©faut (0.3)
**D√©cision:** `temperature: 0.3` pour plus de d√©terminisme
**Raison:** Pour l'extraction de CV, on veut de la coh√©rence pas de cr√©ativit√©
**Impact:** R√©sultats plus reproductibles

### 4. Retries automatiques (3x)
**D√©cision:** R√©essayer automatiquement en cas d'erreur JSON
**Raison:** Mistral g√©n√®re occasionnellement du JSON invalide
**Impact:** Plus r√©silient mais +latence en cas d'erreur

### 5. Parsing JSON robuste
**D√©cision:** Chercher les limites du JSON au lieu de parser directement
**Raison:** Mistral peut retourner du texte avant/apr√®s le JSON
**Impact:** Plus flexible et forgiving

---

## Points d'extension futurs

### 1. Cache des r√©sultats
```python
from functools import lru_cache

@lru_cache(maxsize=100)
def analyze_cv(text):
    # Impl√©menter le cache
    pass
```

### 2. Support de plusieurs mod√®les
```python
class MistralCVAnalyzer:
    def __init__(self, model="mistral"):
        self.model_name = model
    
    def switch_model(self, new_model):
        # Changer de mod√®le
        pass
```

### 3. Streaming de r√©ponses
```python
def analyze_cv_stream(text):
    # Impl√©menter le streaming pour grandes r√©ponses
    pass
```

### 4. Batch processing avec queue
```python
from queue import Queue
from threading import Thread

class BatchAnalyzer:
    def __init__(self, max_workers=4):
        self.queue = Queue()
        self.workers = [Thread(target=self._worker) for _ in range(max_workers)]
```

### 5. Support de plusieurs formats d'output
```python
class MistralCVAnalyzer:
    def analyze_cv(self, text, format="json"):
        # Support json, xml, csv, etc.
        pass
```

---

## Optimisations possibles

### Performance
- [ ] Cache LRU pour les CVs similaires
- [ ] Utiliser async/await pour les appels Ollama
- [ ] Pipeline de traitement parall√®le
- [ ] Quantization du mod√®le (INT8)
- [ ] Model distillation

### Fiabilit√©
- [ ] Circuit breaker pour Ollama
- [ ] Health checks p√©riodiques
- [ ] Logging structur√© (JSON)
- [ ] Metrics Prometheus
- [ ] Fallback sur analyse classique

### Scalabilit√©
- [ ] Load balancer devant plusieurs Ollama
- [ ] Cache distribu√© (Redis)
- [ ] Queue de traitement (Celery)
- [ ] Database pour les r√©sultats
- [ ] API Webhook pour notifications

---

## Consid√©rations de s√©curit√©

### Input Validation
```python
def analyze_cv(text: str) -> Optional[Dict]:
    # Valider la taille
    if len(text) > 100000:
        raise ValueError("CV trop volumineux")
    
    # Valider le contenu
    if not text.strip():
        raise ValueError("CV vide")
```

### Rate Limiting
```python
from datetime import datetime, timedelta

class RateLimiter:
    def __init__(self, max_requests=10, window=60):
        self.max_requests = max_requests
        self.window = window
        self.requests = []
    
    def is_allowed(self):
        now = datetime.now()
        self.requests = [r for r in self.requests 
                        if now - r < timedelta(seconds=self.window)]
        
        if len(self.requests) < self.max_requests:
            self.requests.append(now)
            return True
        return False
```

### Sanitization
```python
def sanitize_cv(text: str) -> str:
    # Retirer les caract√®res dangereux
    # Normaliser les espaces
    # Limiter la longueur
    pass
```

---

## Testing Strategy

### Unit Tests
- [x] Parser JSON
- [x] Prompt building
- [x] Error handling
- [x] Initialization

### Integration Tests
- [ ] Communication Ollama (skip si offline)
- [ ] Full pipeline
- [ ] Error scenarios

### Performance Tests
- [ ] Temps de r√©ponse
- [ ] Utilisation m√©moire
- [ ] Throughput

### Load Tests
- [ ] Multiple concurrent requests
- [ ] Memory leaks
- [ ] Connection pooling

---

## Known Issues & Workarounds

### Issue 1: JSON invalide de Mistral
**Problem:** Mistral g√©n√®re parfois du JSON invalide
**Workaround:** Retries automatiques (impl√©ment√©)
**Future:** Fine-tuning sur JSON structur√©

### Issue 2: Premi√®re requ√™te lente
**Problem:** Warm-up du mod√®le ~5-10s
**Workaround:** Documenter, informer l'utilisateur
**Future:** Keep-alive connection

### Issue 3: Timeout sur CVs tr√®s longs
**Problem:** Timeouts avec CVs >10000 caract√®res
**Workaround:** Augmenter timeout dans `.env.mistral`
**Future:** Chunking automatique

### Issue 4: Ollama crash occasionnel
**Problem:** Ollama peut crash sous charge
**Workaround:** Restart automatique + health checks
**Future:** Isoler Ollama en service systemd

---

## Monitoring & Logging

### Logs √† impl√©menter
```python
import logging

logger = logging.getLogger(__name__)

# Levels
logger.debug("Calling Ollama...")      # Verbeux
logger.info("Analysis successful")      # Infos
logger.warning("Retry attempt 2/3")     # Attentions
logger.error("Connection failed")       # Erreurs
logger.critical("Service down")         # Critiques
```

### Metrics √† tracker
- Nombre d'analyses par jour
- Temps moyen par analyse
- Taux de succ√®s/erreur
- Taille moyenne des CVs
- Latence de r√©ponse Ollama
- Utilisation m√©moire/CPU

### Alertes recommand√©es
- Ollama down
- Taux d'erreur > 5%
- Latence > 120s
- Utilisation m√©moire > 80%
- Aucune requ√™te en 30 minutes

---

## Roadmap

### Phase 1 (‚úÖ Compl√©t√©e)
- [x] Module Mistral de base
- [x] Routes Flask
- [x] Documentation
- [x] Tests de base
- [x] Exemples

### Phase 2 (√Ä faire)
- [ ] Cache des r√©sultats
- [ ] Async/await
- [ ] Metrics Prometheus
- [ ] Logging structur√©
- [ ] Health checks

### Phase 3 (√Ä faire)
- [ ] Support multi-mod√®les
- [ ] Batch processing
- [ ] Pipeline d'inf√©rence
- [ ] Fine-tuning
- [ ] Model distillation

### Phase 4 (√Ä faire)
- [ ] Load balancing
- [ ] Cache distribu√©
- [ ] Queue de traitement
- [ ] Monitoring avanc√©
- [ ] Auto-scaling

---

## Standards & Conventions

### Code Style
```python
# PEP 8
# - 80 caract√®res max par ligne
# - 4 espaces d'indentation
# - snake_case pour fonctions/variables
# - UPPER_CASE pour constantes

# Type hints
def analyze_cv(text: str) -> Optional[Dict[str, Any]]:
    """Docstring avec description."""
    pass
```

### Naming Conventions
```
Classes:        PascalCase (MistralCVAnalyzer)
Functions:      snake_case (analyze_cv)
Constants:      UPPER_SNAKE_CASE (MAX_RETRIES)
Private:        _prefix (_call_ollama)
Protected:      __dunder (si vraiment n√©cessaire)
```

### Documentation
```python
def analyze_cv(text: str) -> Optional[Dict[str, Any]]:
    """
    Analyse un CV avec Mistral et retourne JSON structur√©.
    
    Args:
        text: Texte du CV √† analyser
        
    Returns:
        Dict contenant les informations structur√©es, ou None en cas d'erreur
        
    Raises:
        ValueError: Si le texte est vide
        ConnectionError: Si Ollama n'est pas accessible
        
    Examples:
        >>> result = analyze_cv("Jean Dupont...")
        >>> result['identite']['nom']
        'Dupont'
    """
    pass
```

---

## D√©ploiement

### D√©pendances syst√®me
- Python 3.8+
- Ollama (accessible sur localhost:11434)
- ~4 GB RAM
- ~10 GB disque (pour Mistral)

### V√©rification pr√©-d√©ploiement
```bash
# Tests
python -m pytest backend/test_mistral.py

# Verification
python backend/startup.py

# Health check
python quick_test.py
```

### Variables d'environnement
```bash
export OLLAMA_HOST=http://localhost:11434
export MISTRAL_TEMPERATURE=0.3
export MISTRAL_MAX_RETRIES=3
```

### Docker (optionnel)
```dockerfile
FROM python:3.9

RUN pip install -r requirements.txt

# Ollama doit tourner en-dehors du container
ENV OLLAMA_HOST=http://ollama:11434

CMD ["python", "api.py"]
```

---

## Troubleshooting Guide

### Probl√®me: ImportError
```
Solution:
1. V√©rifier les chemins Python
2. V√©rifier que backend/ est dans sys.path
3. Lancer depuis le dossier backend
```

### Probl√®me: Connection refused
```
Solution:
1. V√©rifier qu'Ollama tourne: ollama serve
2. V√©rifier le port: lsof -i :11434
3. V√©rifier le firewall
```

### Probl√®me: JSON parse error
```
Solution:
1. Retries automatiques g√®rent cela
2. V√©rifier les logs pour plus de d√©tails
3. Augmenter max_retries si n√©cessaire
```

### Probl√®me: Out of memory
```
Solution:
1. Fermer les autres applications
2. Utiliser un mod√®le quantis√© (INT8)
3. Augmenter la RAM syst√®me
4. Chunking des CVs longs
```

---

## Performance Tips

### Pour l'utilisateur
- Garder Ollama actif
- Premi√®re requ√™te peut √™tre lente
- Les suivantes sont rapides
- Fermer autres applications

### Pour le d√©veloppeur
- Cache des r√©sultats fr√©quents
- Batch processing pour plusieurs CVs
- Utiliser GPU si disponible
- Quantization du mod√®le

### Pour l'infra
- Load balancing
- Keep-alive connections
- Connection pooling
- Resource monitoring

---

## Resources & References

### Documentation officielles
- https://ollama.ai/
- https://mistral.ai/
- https://github.com/ollama/ollama

### Code samples
- [mistral_analyzer.py](backend/extractors/mistral_analyzer.py)
- [examples_mistral.py](backend/examples_mistral.py)
- [test_mistral.py](backend/test_mistral.py)

### Guides
- [MISTRAL_GUIDE.md](../Docs/MISTRAL_GUIDE.md)
- [ARCHITECTURE.md](../ARCHITECTURE.md)
- [INTEGRATION_CHECKLIST.md](../INTEGRATION_CHECKLIST.md)

---

## Contact & Support

Pour questions/probl√®mes:
1. Consultez la documentation
2. V√©rifiez les logs
3. Lancez les tests
4. Lisez les exemples

---

**Document de d√©veloppement - Mistral 7B Integration**  
*Derni√®re mise √† jour: 2024*
