# Guide Complet d'Int√©gration de Mistral 7B avec Ollama

## üìã Vue d'ensemble

Ce projet int√®gre **Mistral 7B Instruct** en mode local via **Ollama**. Aucune API externe n'est utilis√©e - tout fonctionne sur votre machine.

## üöÄ Installation Rapide

### √âtape 1: Installer Ollama

#### Windows
1. Allez sur https://ollama.ai/download/windows
2. T√©l√©chargez et ex√©cutez l'installateur
3. Suivez les instructions

#### macOS
```bash
brew install ollama
```

Ou t√©l√©chargez depuis https://ollama.ai/download/mac

#### Linux
```bash
curl https://ollama.ai/install.sh | sh
```

### √âtape 2: T√©l√©charger Mistral

Ouvrez un terminal et ex√©cutez:
```bash
ollama pull mistral
```

Cela t√©l√©charge le mod√®le Mistral 7B Instruct (~4 GB).

### √âtape 3: Lancer Ollama

Gardez un terminal ouvert avec:
```bash
ollama serve
```

Ollama doit rester en arri√®re-plan pour que le projet fonctionne.

### √âtape 4: Utiliser dans le projet

#### Option A: Utiliser le module directement

```python
from extractors.mistral_analyzer import analyze_cv

# Votre texte CV
cv_text = "..."

# Analyser
result = analyze_cv(cv_text)

if result:
    print(result)  # JSON structur√©
else:
    print("Erreur lors de l'analyse")
```

#### Option B: Via l'API Flask

```bash
# V√©rifier le statut
curl http://localhost:5000/api/mistral/status

# Analyser un CV
curl -X POST http://localhost:5000/api/mistral/analyze \
  -H "Content-Type: application/json" \
  -d '{
    "cv_text": "Jean Dupont... (texte du CV)"
  }'
```

#### Option C: Utiliser le script de setup automatis√©

```bash
python backend/setup_ollama.py
```

## üìÅ Structure des fichiers

```
backend/
‚îú‚îÄ‚îÄ extractors/
‚îÇ   ‚îî‚îÄ‚îÄ mistral_analyzer.py      # Module principal d'analyse
‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îî‚îÄ‚îÄ mistral_routes.py         # Endpoints Flask pour Mistral
‚îú‚îÄ‚îÄ setup_ollama.py               # Script de setup automatis√©
‚îî‚îÄ‚îÄ requirements.txt              # D√©pendances Python
```

## üîß Configuration

### Changer l'URL d'Ollama

Par d√©faut, le code utilise `http://localhost:11434` (URL par d√©faut d'Ollama).

Pour utiliser une autre URL:
```python
from extractors.mistral_analyzer import MistralCVAnalyzer

analyzer = MistralCVAnalyzer(ollama_host="http://192.168.1.100:11434")
result = analyzer.analyze_cv(cv_text)
```

### Ajuster les param√®tres du mod√®le

Dans `mistral_analyzer.py`, fonction `_call_ollama()`:

```python
request_data = {
    "model": "mistral",
    "prompt": prompt,
    "stream": False,
    "temperature": 0.3,      # Baisse = plus d√©terministe
    "top_p": 0.9,            # (Optionnel)
    "top_k": 40,             # (Optionnel)
}
```

## üìä Structure JSON de sortie

Le mod√®le retourne:

```json
{
  "identite": {
    "nom": "Dupont",
    "prenom": "Jean"
  },
  "contact": {
    "adresse": "123 Rue de Paris",
    "ville": "Paris",
    "code_postal": "75001",
    "email": "jean@example.com",
    "telephone": "+33612345678"
  },
  "experience": [
    {
      "poste": "D√©veloppeur Python",
      "entreprise": "Acme Corp",
      "ville": "Paris",
      "date_debut": "2020-01-15",
      "date_fin": "2023-12-31",
      "description": "D√©veloppement d'applications..."
    }
  ],
  "formation": [
    {
      "diplome": "Master Informatique",
      "ecole": "Universit√© X",
      "date_debut": "2016-09-01",
      "date_fin": "2018-06-30"
    }
  ],
  "certifications": ["AWS Solutions Architect"],
  "langues": ["Fran√ßais", "Anglais", "Espagnol"],
  "competences": ["Python", "Django", "Docker", "PostgreSQL"],
  "resume": "D√©veloppeur exp√©riment√© avec..."
}
```

## üß™ Test

### Test du module

```bash
cd backend
python -m extractors.mistral_analyzer
```

### Test du setup

```bash
python backend/setup_ollama.py
```

### Test avec curl

```bash
# V√©rifier la sant√©
curl http://localhost:5000/api/mistral/health

# Analyser
curl -X POST http://localhost:5000/api/mistral/analyze \
  -H "Content-Type: application/json" \
  -d '{"cv_text": "Mon CV..."}'
```

## üêõ D√©pannage

### "Ollama n'est pas accessible"

**Solution:**
1. V√©rifiez qu'Ollama est install√©: `ollama --version`
2. Lancez Ollama: `ollama serve` (dans un autre terminal)
3. V√©rifiez que le port 11434 n'est pas bloqu√©

### "Le mod√®le Mistral n'est pas t√©l√©charg√©"

**Solution:**
```bash
ollama pull mistral
```

Attendez que le t√©l√©chargement (~4 GB) soit complet.

### "Impossible de parser le JSON"

**Cause:** Le mod√®le a g√©n√©r√© une r√©ponse invalide

**Solutions:**
1. V√©rifiez que le CV est bien format√©
2. R√©essayez - Mistral peut g√©n√©rer du JSON valide au prochain essai
3. Augmentez le nombre de tentatives dans `MistralCVAnalyzer.max_retries`

### Performance lente

**Causes possibles:**
1. La premi√®re requ√™te peut √™tre lente (warm-up du mod√®le)
2. Votre CPU/GPU n'est pas assez puissant
3. Ollama utilise trop de RAM

**Solutions:**
1. Les requ√™tes suivantes seront plus rapides
2. Fermez d'autres applications
3. Attendez quelques secondes avant la prochaine requ√™te

## üîê S√©curit√©

- ‚úÖ **100% local** - Aucune donn√©e ne quitte votre machine
- ‚úÖ **Open source** - Code Mistral disponible
- ‚úÖ **Sans API** - Aucune cl√© API √† g√©rer
- ‚ö†Ô∏è **√Ä prot√©ger** - Isolez le port 11434 sur un r√©seau public

## üì¶ D√©pendances Python

Aucune d√©pendance Python sp√©ciale requise! Le code n'utilise que la stdlib:
- `json` (parsing JSON)
- `logging` (logs)
- `urllib` (requ√™tes HTTP)
- `subprocess` (lancer les commandes)

## üéØ Prochaines √©tapes

1. **Int√©gration dans l'API:**
   - Ajoutez les blueprints Flask de `mistral_routes.py` √† votre `api.py`:
   ```python
   from routes.mistral_routes import mistral_bp
   app.register_blueprint(mistral_bp)
   ```

2. **Pipeline complet:**
   - Combinez avec votre extraction PDF/DOCX
   - Utilisez `analyze_cv()` sur le texte extrait

3. **Optimisations:**
   - Cache des r√©sultats
   - Queue d'attente pour les analyses longues
   - Support de mod√®les plus petits (quantifi√©s) pour plus de vitesse

## üìö Ressources

- [Ollama](https://ollama.ai/)
- [Mistral 7B](https://mistral.ai/)
- [API Ollama](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Format des prompts](https://docs.mistral.ai/capabilities/function_calling/)

## üìù Notes

- Le mod√®le utilise le format chat Mistral (optimal pour instruction following)
- Temp√©rature √† 0.3 pour plus de d√©terminisme
- Retries automatiques en cas d'erreur
- Logs d√©taill√©s pour le d√©bogage

## ‚ö° Performance attendue

| Configuration | Temps/CV |
|---|---|
| CPU moderne (8+ cores) | 30-60s |
| GPU (RTX 3070+) | 5-15s |
| Premi√®re requ√™te | +5-10s (warm-up) |

Doublez ces estimations si vous lancez d'autres applications.

---

**Besoin d'aide?** V√©rifiez d'abord les logs avec:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```
