# Analyse du Backend - Architecture et ImplÃ©mentation

## ğŸ¯ Vue d'ensemble

Le backend est une API Flask qui orchestre l'extraction, la normalisation et la gÃ©nÃ©ration de documents CV. Il utilise un pipeline robuste combinant regex, NLP (spaCy) et heuristiques intelligentes.

### Stack Technique
- **Framework** : Flask + Flask-CORS
- **NLP** : spaCy (modÃ¨le `fr_core_news_md`)
- **Fuzzy Matching** : rapidfuzz
- **Manipulation docs** : python-docx, PyPDF2, pdfplumber
- **Conversion** : docx2pdf (Windows/Linux)
- **Tests** : pytest

---

## ğŸ“‚ Structure Modulaire

```
backend/
â”œâ”€â”€ api.py                       # ğŸŒ API REST Flask
â”œâ”€â”€ analyser_cv.py               # ğŸ”¬ Script offline
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ extractors/
â”‚   â”œâ”€â”€ robust_extractor.py      # â­ ORCHESTRATEUR PRINCIPAL
â”‚   â”‚   â””â”€ Pipelines 4 niveaux (Regex, spaCy, Heuristiques, Fuzzy)
â”‚   â”‚
â”‚   â”œâ”€â”€ enhanced_extractor.py    # ğŸ” Extraction Regex
â”‚   â”‚   â”œâ”€ extract_email()
â”‚   â”‚   â”œâ”€ extract_phone()
â”‚   â”‚   â”œâ”€ extract_date()
â”‚   â”‚   â””â”€ extract_address()
â”‚   â”‚
â”‚   â”œâ”€â”€ spacy_extractor.py       # ğŸ§  NER (Named Entity Recognition)
â”‚   â”‚   â”œâ”€ Noms (PER)
â”‚   â”‚   â”œâ”€ Organisations (ORG)
â”‚   â”‚   â”œâ”€ Lieux (LOC)
â”‚   â”‚   â””â”€ Dates (DATE)
â”‚   â”‚
â”‚   â”œâ”€â”€ heuristic_rules.py       # ğŸ¯ RÃ¨gles Intelligentes
â”‚   â”‚   â”œâ”€ Classification Formation/ExpÃ©rience
â”‚   â”‚   â”œâ”€ Association dates-entreprises
â”‚   â”‚   â””â”€ DÃ©tection contexte
â”‚   â”‚
â”‚   â”œâ”€â”€ section_classifier.py    # ğŸ§© Finalisation
â”‚   â”‚   â”œâ”€ Fuzzy matching
â”‚   â”‚   â”œâ”€ DÃ©duplication
â”‚   â”‚   â””â”€ Construction JSON final
â”‚   â”‚
â”‚   â”œâ”€â”€ version_mapper.py        # ğŸ”„ Conversion formats
â”‚   â”‚   â”œâ”€ normalize_old_cv_to_new()
â”‚   â”‚   â””â”€ convert_v2_to_old_format()
â”‚   â”‚
â”‚   â””â”€â”€ config.py                # âš™ï¸ Configuration centralisÃ©e
â”‚
â”œâ”€â”€ generators/
â”‚   â”œâ”€â”€ generate_sopra_docx.py   # ğŸ“ GÃ©nÃ©ration DOCX
â”‚   â”‚   â””â”€ Formatage template Sopra
â”‚   â”‚
â”‚   â””â”€â”€ docx_to_pdf.py           # ğŸ“„ Conversion DOCX â†’ PDF
â”‚       â””â”€ Utilise docx2pdf + pythoncom (Windows)
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ cv_ner/                  # ModÃ¨le NER personnalisÃ©
â”‚   â””â”€â”€ cv_pipeline/             # Pipeline spaCy complet
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train_ner.py             # EntraÃ®nement NER
â”‚   â”œâ”€â”€ train_pipeline.py        # EntraÃ®nement pipeline
â”‚   â”œâ”€â”€ train_textcat.py         # Classification texte
â”‚   â”œâ”€â”€ generate_training_data.py
â”‚   â””â”€â”€ training_data.py
â”‚
â””â”€â”€ data/
    â”œâ”€â”€ input/                   # CVs uploadÃ©s
    â””â”€â”€ output/                  # JSON gÃ©nÃ©rÃ©s
```

---

## ğŸ”Œ API Endpoints

### POST `/api/cv/analyze`

**Analyse un CV et retourne JSON structurÃ©**

```
Input  : FormData { file: CV.pdf ou CV.docx }
Output : JSON { contact, experiences, formations, competences, langues }
Status : 200 OK | 400 Bad Request | 500 Error
```

**Flux interne** :
```
1. Validation fichier
2. Stockage temporaire (data/input/)
3. robust_extractor.extract_cv_robust()
4. Sauvegarde JSON (data/output/)
5. Retour rÃ©ponse API
```

### GET `/api/cv/json/<filename>`

**TÃ©lÃ©charge le JSON gÃ©nÃ©rÃ©**

```
Input  : filename (ex: "CV_Jean_Dupont.json")
Output : Fichier JSON binaire
```

### POST `/api/cv/generate-docx`

**GÃ©nÃ¨re un DOCX depuis JSON**

```
Input  : JSON (body ou reference)
Output : DOCX au format Sopra Steria
```

### POST `/api/cv/convert-docx-to-pdf`

**Convertit DOCX en PDF**

```
Input  : DOCX file ou path
Output : PDF (data/output/)
```

---

## ğŸ§  Pipeline d'Extraction DÃ©taillÃ©

### Niveau 1ï¸âƒ£ : REGEX EXTRACTION

**Fichier** : `enhanced_extractor.py`

```python
# Emails
pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'

# TÃ©lÃ©phones (FR)
patterns = [
    r'\+33\s?[1-9](?:\s?\d{2}){4}',      # +33 6 12 34 56 78
    r'0[1-9](?:\s?\d{2}){4}',            # 06 12 34 56 78
    r'0[1-9]\.?\d{2}\.?\d{2}\.?\d{2}\.?\d{2}'  # 06.12.34.56.78
]

# Dates (Multiples formats)
patterns = [
    r'\b(0[1-9]|1[012])[-/]((?:19|20)\d{2})\b',  # MM/YYYY
    r'((?:19|20)\d{2})\s*[-â€“]\s*((?:19|20)\d{2})',  # YYYY-YYYY
    r'(?:Janvier|FÃ©vrier|...)\s+((?:19|20)\d{2})'  # Mois YYYY
]

# Adresses
pattern = r'(\d+\s+(?:rue|avenue|boulevard|route|chemin).*?\d{5})'
```

**RÃ©sultat** : Dict avec clÃ©s `emails`, `phones`, `dates`, `addresses`

### Niveau 2ï¸âƒ£ : SPACY NER

**Fichier** : `spacy_extractor.py`

```python
import spacy

nlp = spacy.load('fr_core_news_md')
doc = nlp(texte)

for ent in doc.ents:
    if ent.label_ == 'PER':      # Noms
        names.append(ent.text)
    elif ent.label_ == 'ORG':    # Organisations
        orgs.append(ent.text)
    elif ent.label_ == 'LOC':    # Lieux
        locations.append(ent.text)
    elif ent.label_ == 'DATE':   # Dates
        dates.append(ent.text)
```

**Fallback** : Si NER insuffisant, utilise regex avancÃ©e

**RÃ©sultat** : Dict avec clÃ©s `persons`, `organizations`, `locations`, `dates`

### Niveau 3ï¸âƒ£ : HEURISTIC RULES

**Fichier** : `heuristic_rules.py`

```python
# Classification Formation vs ExpÃ©rience
FORMATION_KEYWORDS = ['diplÃ´me', 'master', 'licence', 'Ã©cole', 'universitÃ©']
EXPERIENCE_KEYWORDS = ['poste', 'dÃ©veloppeur', 'responsable', 'manager']

# Association dates â†” entreprises
def link_date_to_org(text, date, org):
    distance = text.find(org) - text.find(date)
    if -1000 < distance < 1000:  # ProximitÃ© textuelle
        return True
    return False

# DÃ©tection type d'emploi
def detect_job_type(title):
    if 'senior' in title.lower():
        return 'Senior'
    elif 'junior' in title.lower():
        return 'Junior'
    else:
        return 'IntermÃ©diaire'
```

**RÃ©sultat** : Sections structurÃ©es (formations, expÃ©riences, compÃ©tences)

### Niveau 4ï¸âƒ£ : FUZZY MATCHING

**Fichier** : `section_classifier.py`

```python
from rapidfuzz import fuzz

# Grouper doublons
if fuzz.ratio(item1, item2) > 80:  # 80% similitude
    merge(item1, item2)

# Normaliser entreprises
'Amazon Inc' ~ 'amazon.com' ~ 'AMAZON'  â†’ 'Amazon'
'SociÃ©tÃ© GÃ©nÃ©rale' ~ 'SG' ~ 'SocGen'    â†’ 'SociÃ©tÃ© GÃ©nÃ©rale'
```

**RÃ©sultat** : JSON final propre et dÃ©dupliquÃ©

---

## ğŸ“Š Flux DÃ©taillÃ© : process_cv()

```python
def process_cv(file_path):
    # 1. DÃ©tecter format
    if file_path.endswith('.pdf'):
        texte = extract_text_from_pdf(file_path)
    else:
        texte = extract_text_from_docx(file_path)
    
    # 2. Appeler robust_extractor
    resultats = extract_cv_robust(texte)
    
    # 3. Sauvegarder JSON
    nom = resultats['contact']['nom']
    json_path = f"data/output/CV_{nom}.json"
    with open(json_path, 'w') as f:
        json.dump(resultats, f, indent=2)
    
    # 4. Retourner rÃ©sultats
    return resultats
```

---

## ğŸ›¡ï¸ Gestion des Erreurs

```python
try:
    resultats = extract_cv_robust(str(file_path))
except PDFException as e:
    return {"error": "PDF corrompu ou non lisible"}, 500
except ValueError as e:
    return {"error": "Extraction Ã©chouÃ©e"}, 500
except Exception as e:
    logging.error(f"Erreur inconnue: {str(e)}")
    return {"error": "Erreur serveur"}, 500
```

---

## âš™ï¸ Configuration (config.py)

```python
# ModÃ¨les et chemins
SPACY_MODEL = 'fr_core_news_md'
MODEL_PATH = 'models/cv_ner'

# Formats acceptÃ©s
ALLOWED_EXTENSIONS = {'pdf', 'docx'}
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB

# Seuils
FUZZY_THRESHOLD = 80          # % similitude
DATE_PROXIMITY = 1000         # caractÃ¨res
MIN_CONFIDENCE = 0.7          # Confiance extraction

# Chemins
DATA_INPUT = 'data/input'
DATA_OUTPUT = 'data/output'
TEMPLATES_PATH = 'templates'
```

---

## ğŸ§ª Tests Unitaires

```bash
# Test extraction noms
pytest test_nom_prenom.py -v
â”œâ”€ test_extract_firstname()
â”œâ”€ test_extract_lastname()
â””â”€ test_extract_middle_name()

# Test extraction adresses
pytest test_cas_rue.py -v
â”œâ”€ test_extract_simple_address()
â”œâ”€ test_extract_postal_code()
â””â”€ test_extract_complex_address()

# Test CV complet
pytest test_cv.py -v
â”œâ”€ test_extract_contact()
â”œâ”€ test_extract_experiences()
â””â”€ test_extract_formations()

# Test intÃ©gration
pytest test_integration.py -v
â””â”€ test_full_pipeline()
```

---

## ğŸš€ Performance et Optimisations

| Aspect | Optimisation | Impact |
|--------|-------------|--------|
| **Regex** | Compilation prÃ©alable | -50% temps |
| **spaCy** | Chargement unique | -60% mÃ©moire |
| **Fuzzy matching** | LimitÃ© aux simiâ‰¥70% | -80% temps |
| **Cache** | JSON en mÃ©moire | +100% rapiditÃ© |

---

## ğŸ”® Extensions Futures

- [ ] **OCR** : Support PDF scannÃ©s (Tesseract)
- [ ] **Multilingue** : ModÃ¨les EN, ES, DE
- [ ] **ML avancÃ©** : Classification multiclass
- [ ] **API async** : FastAPI pour scalabilitÃ©
- [ ] **Cache Redis** : Pour modÃ¨les lourds
- [ ] **Webhooks** : Notifications post-analyse
