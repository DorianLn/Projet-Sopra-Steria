# ‚úÖ CHECKLIST D'INT√âGRATION - Mistral 7B

## Phase 1: Installation syst√®me (10-30 minutes)

### √âtape 1.1: Installer Ollama
- [ ] T√©l√©charger Ollama depuis https://ollama.ai/download
- [ ] Ex√©cuter l'installer
- [ ] V√©rifier l'installation: `ollama --version`

**Resources:**
- Windows: https://ollama.ai/download/windows
- macOS: https://ollama.ai/download/mac
- Linux: `curl https://ollama.ai/install.sh | sh`

### √âtape 1.2: Lancer Ollama
- [ ] Ouvrir un terminal
- [ ] Ex√©cuter: `ollama serve`
- [ ] Garder le terminal ouvert en arri√®re-plan

### √âtape 1.3: T√©l√©charger Mistral
- [ ] Ouvrir un NOUVEAU terminal
- [ ] Ex√©cuter: `ollama pull mistral`
- [ ] Attendre la fin du t√©l√©chargement (~4 GB)
- [ ] V√©rifier: `ollama list` (Mistral doit appara√Ætre)

---

## Phase 2: V√©rification du projet (5 minutes)

### √âtape 2.1: V√©rifier les fichiers cr√©√©s
- [ ] `backend/extractors/mistral_analyzer.py` existe
- [ ] `backend/routes/mistral_routes.py` existe
- [ ] `backend/setup_ollama.py` existe
- [ ] `backend/startup.py` existe
- [ ] Documentation existe:
  - [ ] `MISTRAL_QUICKSTART.md`
  - [ ] `Docs/MISTRAL_GUIDE.md`
  - [ ] `backend/INTEGRATION_MISTRAL.md`

### √âtape 2.2: V√©rifier l'environment Python
- [ ] Python 3.8+ install√©: `python --version`
- [ ] Environment virtuel actif: `(venv)` visible dans le terminal
- [ ] D√©pendances install√©es: `pip install -r requirements.txt`

### √âtape 2.3: Tester Mistral seul
```bash
python -c "from extractors.mistral_analyzer import verify_mistral_setup; print(verify_mistral_setup())"
```
- [ ] R√©sultat: `'status': 'OK'`

---

## Phase 3: Test du module (10 minutes)

### √âtape 3.1: Test simple
```bash
python -c "
from extractors.mistral_analyzer import analyze_cv
result = analyze_cv('Jean Dupont\nEmail: jean@example.com\nD√©veloppeur')
print(result)
"
```
- [ ] R√©sultat: JSON structur√© avec identit√©, contact, etc.

### √âtape 3.2: Lancer les exemples
```bash
python backend/examples_mistral.py
```
- [ ] R√©sultat: 7 exemples fonctionnent sans erreur

### √âtape 3.3: Lancer les tests
```bash
python -m pytest backend/test_mistral.py -v
```
- [ ] R√©sultat: Tous les tests passent

---

## Phase 4: Int√©gration API Flask (10 minutes)

### √âtape 4.1: Ajouter les imports
√âditer `backend/api.py`:
```python
# Ajouter apr√®s les autres imports:
from extractors.mistral_analyzer import analyze_cv as mistral_analyze_cv
from routes.mistral_routes import mistral_bp
```
- [ ] Imports ajout√©s

### √âtape 4.2: Enregistrer le blueprint
√âditer `backend/api.py` (apr√®s `CORS(app)`):
```python
# Enregistrer les routes Mistral
app.register_blueprint(mistral_bp)
```
- [ ] Blueprint enregistr√©

### √âtape 4.3: Tester les endpoints Mistral
```bash
# Terminal 1: Lancer l'API
python backend/api.py

# Terminal 2: Tester
curl http://localhost:5000/api/mistral/status
curl -X POST http://localhost:5000/api/mistral/analyze \
  -H "Content-Type: application/json" \
  -d '{"cv_text": "Jean Dupont..."}'
```
- [ ] Endpoints r√©pondent correctement

---

## Phase 5: Startup complet (5 minutes)

### √âtape 5.1: Tester le startup
```bash
python backend/startup.py
```
- [ ] Toutes les v√©rifications passent
- [ ] API d√©marre avec succ√®s

### √âtape 5.2: Acc√©der √† l'API
- [ ] Ouvrir http://localhost:5000
- [ ] V√©rifier que l'API fonctionne

---

## Phase 6: Production (optionnel)

### √âtape 6.1: Logs et monitoring
- [ ] Configurer les logs: `LOG_LEVEL=INFO` dans `.env.mistral`
- [ ] Tester les logs: `python maintenance.py` ‚Üí Option 9

### √âtape 6.2: Sauvegarde automatique
- [ ] `AUTO_SAVE_RESULTS=true` dans `.env.mistral`
- [ ] Dossier `backend/data/output` existe

### √âtape 6.3: Maintenance
- [ ] Lancer le menu maintenance: `python backend/maintenance.py`
- [ ] Tester le nettoyage des fichiers temporaires

---

## Tests de validation finale

### ‚úÖ Test 1: Module seul
```python
from extractors.mistral_analyzer import analyze_cv

cv = """
Jean Dupont
jean@example.com
06 12 34 56 78
D√©veloppeur Python - 5 ans d'exp√©rience
"""

result = analyze_cv(cv)
assert result is not None
assert "identite" in result
assert "contact" in result
print("‚úì Module fonctionne")
```

### ‚úÖ Test 2: API Flask
```bash
curl -X POST http://localhost:5000/api/mistral/analyze \
  -H "Content-Type: application/json" \
  -d '{"cv_text": "Jean Dupont"}'
```
- R√©ponse: `{"success": true, "data": {...}}`

### ‚úÖ Test 3: V√©rification setup
```python
from extractors.mistral_analyzer import verify_mistral_setup
status = verify_mistral_setup()
assert status['status'] == 'OK'
```

### ‚úÖ Test 4: Avec fichier CV
```bash
# Cr√©er un fichier test.txt avec un CV
echo "Jean Dupont, D√©veloppeur" > test.txt

# Analyser
python -c "
from extractors.mistral_analyzer import analyze_cv
with open('test.txt') as f:
    result = analyze_cv(f.read())
    print(result)
"
```

---

## D√©pannage

### ‚ùå "Ollama n'est pas accessible"
- [ ] Ollama est install√©: `ollama --version`
- [ ] Ollama tourne: `ollama serve` dans un autre terminal
- [ ] Port 11434 n'est pas bloqu√©

### ‚ùå "Mistral n'est pas t√©l√©charg√©"
- [ ] Ex√©cuter: `ollama pull mistral`
- [ ] V√©rifier: `ollama list` (Mistral doit y √™tre)

### ‚ùå "ImportError: No module named 'extractors'"
- [ ] V√©rifier le chemin: √ätre dans le dossier `backend`
- [ ] `sys.path` contient le r√©pertoire courant

### ‚ùå "JSON parsing error"
- [ ] Mistral peut g√©n√©rer du JSON invalide parfois
- [ ] Retries automatiques (3 fois par d√©faut)
- [ ] V√©rifier les logs: `python maintenance.py` ‚Üí Option 9

### ‚ùå "Request timeout"
- [ ] Mistral peut √™tre lent la premi√®re fois
- [ ] Attendre 2-3 minutes
- [ ] Aumenter OLLAMA_TIMEOUT dans `.env.mistral`

---

## Optimisations optionnelles

### Performance
- [ ] GPU activ√© pour Ollama (si disponible)
- [ ] Temp√©rature ajust√©e (0.3 = d√©terministe)
- [ ] Cache des r√©sultats?

### Fiabilit√©
- [ ] Retries augment√©s si n√©cessaire
- [ ] Timeouts adapt√©s √† votre infrastructure
- [ ] Fallback vers analyse classique?

### S√©curit√©
- [ ] Port 11434 isol√© sur r√©seau public
- [ ] Validation des inputs
- [ ] Logs d√©taill√©s pour audit

---

## Ressources

üìö Documentation cr√©√©e:
- `MISTRAL_QUICKSTART.md` - D√©marrage rapide (5 min)
- `Docs/MISTRAL_GUIDE.md` - Guide complet (30 min)
- `backend/INTEGRATION_MISTRAL.md` - Int√©gration API
- `backend/examples_mistral.py` - 7 exemples complets
- `backend/test_mistral.py` - Tests unitaires

üîó Liens utiles:
- Ollama: https://ollama.ai/
- Mistral: https://mistral.ai/
- API Ollama: https://github.com/ollama/ollama/blob/main/docs/api.md

---

## Points importants √† retenir

1. **Ollama doit rester actif** - Gardez `ollama serve` lanc√©
2. **Premi√®re requ√™te lente** - C'est normal (warm-up du mod√®le)
3. **100% local** - Aucune donn√©es n'envoie vers l'ext√©rieur
4. **Pas de d√©pendances** - Utilise seulement stdlib Python
5. **Retries automatiques** - Les erreurs JSON sont g√©r√©es

---

## Apr√®s l'int√©gration

Une fois tout en place, vous pouvez:

‚úÖ Analyser des CVs avec Mistral:
```python
from extractors.mistral_analyzer import analyze_cv
result = analyze_cv(cv_text)
```

‚úÖ Utiliser l'API:
```bash
POST /api/mistral/analyze
```

‚úÖ Combiner avec votre analyse classique:
```python
# Analyse classique + Mistral
```

‚úÖ Monitorer le syst√®me:
```bash
python backend/maintenance.py
```

---

**üéâ Int√©gration termin√©e!**

Votre projet Sopra Steria utilise maintenant Mistral 7B en local avec Ollama.

Besoin d'aide? Consultez la documentation compl√®te:
1. `MISTRAL_QUICKSTART.md` pour un d√©marrage rapide
2. `Docs/MISTRAL_GUIDE.md` pour la documentation compl√®te
3. `backend/examples_mistral.py` pour des exemples
4. `backend/test_mistral.py --manual` pour tester
