# IMPLÃ‰MENTATION RAPIDE - Mistral 7B Instruct

## ğŸš€ Quick Start (5 minutes)

### 1. Installer Ollama
- **Windows**: TÃ©lÃ©charger depuis https://ollama.ai/download/windows
- **macOS**: `brew install ollama`
- **Linux**: `curl https://ollama.ai/install.sh | sh`

### 2. DÃ©marrer Ollama
```bash
ollama serve
```
(Garder le terminal ouvert)

### 3. TÃ©lÃ©charger Mistral
```bash
ollama pull mistral
```

### 4. Utiliser le code
```python
from extractors.mistral_analyzer import analyze_cv

cv_text = "..."  # Texte du CV
result = analyze_cv(cv_text)  # Retourne du JSON structurÃ©
```

## ğŸ“ Fichiers crÃ©Ã©s

```
backend/
â”œâ”€â”€ extractors/
â”‚   â””â”€â”€ mistral_analyzer.py       # Module principal (300+ lignes)
â”‚
â”œâ”€â”€ routes/
â”‚   â””â”€â”€ mistral_routes.py          # Endpoints Flask (80 lignes)
â”‚
â”œâ”€â”€ examples_mistral.py            # 7 exemples d'utilisation (300+ lignes)
â”œâ”€â”€ setup_ollama.py                # Setup automatisÃ© (200+ lignes)
â”œâ”€â”€ startup.py                     # Startup avec vÃ©rifications (300+ lignes)
â”œâ”€â”€ test_mistral.py                # Tests unitaires (250+ lignes)
â”œâ”€â”€ mistral_menu.bat               # Menu Windows
â”œâ”€â”€ .env.mistral                   # Configuration
â”œâ”€â”€ INTEGRATION_MISTRAL.md         # Guide d'intÃ©gration API
â””â”€â”€ MISTRAL_GUIDE.md               # Guide complet

Docs/
â””â”€â”€ MISTRAL_GUIDE.md               # Documentation complÃ¨te
```

## ğŸ¯ Utilisation directe

### Option 1: Simple (sans API)
```python
from extractors.mistral_analyzer import analyze_cv

result = analyze_cv("Jean Dupont\nEmail: jean@example.com\n...")
print(result)  # {'identite': {...}, 'contact': {...}, ...}
```

### Option 2: Avec la classe
```python
from extractors.mistral_analyzer import MistralCVAnalyzer

analyzer = MistralCVAnalyzer()
result = analyzer.analyze_cv(cv_text)
```

### Option 3: Via API Flask
```bash
curl -X POST http://localhost:5000/api/mistral/analyze \
  -H "Content-Type: application/json" \
  -d '{"cv_text": "..."}'
```

## ğŸ“Š Structure JSON retournÃ©e

```json
{
  "identite": {
    "nom": "Dupont",
    "prenom": "Jean"
  },
  "contact": {
    "adresse": "123 Rue de Paris",
    "ville": "Paris",
    "code_postal": "75001",
    "email": "jean@example.com",
    "telephone": "+33612345678"
  },
  "experience": [
    {
      "poste": "DÃ©veloppeur Python",
      "entreprise": "Acme Corp",
      "ville": "Paris",
      "date_debut": "2020-01-15",
      "date_fin": "2023-12-31",
      "description": "..."
    }
  ],
  "formation": [...],
  "certifications": [...],
  "langues": [...],
  "competences": [...],
  "resume": "..."
}
```

## âš™ï¸ Configuration

### Changer l'hÃ´te Ollama
```python
analyzer = MistralCVAnalyzer(ollama_host="http://192.168.1.1:11434")
```

### Ajuster la tempÃ©rature (dÃ©tail/crÃ©ativitÃ©)
- 0.0 = dÃ©terministe (mieux pour extraction)
- 0.5 = Ã©quilibrÃ©
- 1.0 = crÃ©atif

Modifier dans `mistral_analyzer.py`, fonction `_call_ollama()`:
```python
"temperature": 0.3,  # â† modifier ici
```

## ğŸ”§ DÃ©pannage

### "Ollama n'est pas accessible"
```bash
# VÃ©rifier Ollama
ollama --version

# Lancer Ollama
ollama serve
```

### "Mistral non trouvÃ©"
```bash
# TÃ©lÃ©charger
ollama pull mistral
```

### Performance lente
- PremiÃ¨re requÃªte plus lente (warm-up)
- Fermer les autres applications
- Attendre quelques secondes entre les requÃªtes

## âœ… VÃ©rifications

```python
from extractors.mistral_analyzer import verify_mistral_setup

status = verify_mistral_setup()
print(status)
# {
#   'ollama_accessible': True,
#   'mistral_downloaded': True,
#   'status': 'OK',
#   'next_steps': [...]
# }
```

## ğŸ§ª Tests

```bash
# Test simple
python -c "from extractors.mistral_analyzer import analyze_cv; print(analyze_cv('Test'))"

# Tous les tests
python -m pytest backend/test_mistral.py

# Test manuel
python backend/test_mistral.py --manual

# Menu interactif (Windows)
backend/mistral_menu.bat

# Startup complet
python backend/startup.py
```

## ğŸ“ˆ Performances attendues

| Configuration | Temps/CV |
|---|---|
| CPU 8 cores | 30-60s |
| CPU 16 cores | 15-30s |
| GPU RTX 3070+ | 5-15s |
| PremiÃ¨re requÃªte | +5-10s |

## ğŸ” SÃ©curitÃ©

- âœ… 100% local - Aucune donnÃ©e externe
- âœ… Open source - Code transparent
- âœ… Sans API - Pas de clÃ© Ã  gÃ©rer
- âš ï¸ Isoler le port 11434 sur rÃ©seau public

## ğŸ“š Ressources

- [Ollama](https://ollama.ai/)
- [Mistral 7B](https://mistral.ai/)
- [API Ollama](https://github.com/ollama/ollama/blob/main/docs/api.md)

## ğŸ“ Exemples complets

Voir `examples_mistral.py` pour 7 exemples complets:
1. Utilisation simple
2. Avec la classe
3. Depuis un fichier
4. VÃ©rification setup
5. Traitement batch
6. Sauvegarde rÃ©sultats
7. Gestion erreurs

```bash
python backend/examples_mistral.py
```

## ğŸš¦ IntÃ©gration Ã  votre API

Voir `INTEGRATION_MISTRAL.md` pour les snippets Ã  ajouter Ã  `api.py`:
- Endpoint `/api/mistral/analyze` pour texte direct
- Endpoint `/api/mistral/status` pour vÃ©rifier le setup
- Endpoint `/api/cv/analyze-hybrid` pour combiner extraction classique + Mistral

## ğŸ“ Notes importantes

1. **Ollama doit rester actif** - Gardez `ollama serve` lancÃ©
2. **PremiÃ¨re requÃªte plus lente** - C'est normal (warm-up du modÃ¨le)
3. **~4 GB de RAM** nÃ©cessaires pour Mistral 7B
4. **Erreurs JSON** - Mistral peut gÃ©nÃ©rer du JSON invalide, retries automatiques
5. **Langue mixte** - Fonctionne avec CV en franÃ§ais, anglais, etc.

## âœ¨ Points forts

- âœ… Vraiment local (100% privÃ©)
- âœ… Pas de dÃ©pendances externes (urllib seulement)
- âœ… Gestion d'erreurs robuste
- âœ… Retries automatiques
- âœ… Logging dÃ©taillÃ©
- âœ… PrÃªt pour production
- âœ… Code Python pur (2 fichiers principaux)

---

**Besoin d'aide?** VÃ©rifiez les logs:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```
