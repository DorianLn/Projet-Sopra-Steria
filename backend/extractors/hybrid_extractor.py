"""
Extracteur HYBRIDE : Combine robust_extractor + spaCy ML

Pipeline intelligent :
1. Essayer extraction par r√®gles (robust_extractor)
2. Valider le r√©sultat avec is_valid_extraction()
3. Si invalide ‚Üí utiliser model_based_extraction avec spaCy entra√Æn√©
4. Retourner le meilleur r√©sultat

Avantages :
- Garde la performance des r√®gles pour CV bien structur√©s (Leo, JLA)
- Utilise le ML pour les CV mal structur√©s (Ad√®le)
- 100% compatible avec le format JSON existant
- Pas de r√©entra√Ænement n√©cessaire
"""

import re
import spacy
import logging
from pathlib import Path
from typing import Dict, List, Any, Tuple
from collections import defaultdict

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# =============================================================================
# CONFIGURATION
# =============================================================================

# Chemin vers le mod√®le entra√Æn√©
TRAINED_MODEL_PATH = Path(__file__).parent.parent / "models" / "cv_ner"
BACKUP_MODEL_PATH = Path(__file__).parent.parent / "models" / "cv_pipeline"
BASE_MODEL = "fr_core_news_md"

# Cache du mod√®le spaCy
_nlp_cache = None

# =============================================================================
# CHARGEMENT DU MOD√àLE SPACY
# =============================================================================

def load_spacy_model():
    """Charge le mod√®le spaCy entra√Æn√© ou fallback."""
    global _nlp_cache

    if _nlp_cache is not None:
        return _nlp_cache

    # Essayer le mod√®le principal
    if TRAINED_MODEL_PATH.exists():
        try:
            _nlp_cache = spacy.load(str(TRAINED_MODEL_PATH))
            logger.info(f"‚úì Mod√®le cv_ner charg√© depuis {TRAINED_MODEL_PATH}")
            return _nlp_cache
        except Exception as e:
            logger.warning(f"Erreur chargement cv_ner: {e}")

    # Essayer le mod√®le backup
    if BACKUP_MODEL_PATH.exists():
        try:
            _nlp_cache = spacy.load(str(BACKUP_MODEL_PATH))
            logger.info(f"‚úì Mod√®le cv_pipeline charg√© depuis {BACKUP_MODEL_PATH}")
            return _nlp_cache
        except Exception as e:
            logger.warning(f"Erreur chargement cv_pipeline: {e}")

    # Fallback vers le mod√®le standard
    try:
        _nlp_cache = spacy.load(BASE_MODEL)
        logger.info(f"‚úì Mod√®le standard {BASE_MODEL} charg√© (fallback)")
        return _nlp_cache
    except OSError as e:
        logger.error(f"ERREUR: Aucun mod√®le disponible. {BASE_MODEL} non trouv√©.")
        raise RuntimeError(
            f"Impossible de charger un mod√®le spaCy. "
            f"Installez avec: python -m spacy download {BASE_MODEL}"
        ) from e


# =============================================================================
# VALIDATION DE L'EXTRACTION
# =============================================================================

def is_valid_extraction(data: Dict[str, Any]) -> bool:
    """
    Valide si l'extraction a r√©ussi selon les crit√®res minimums.

    Crit√®res :
    - contact.nom doit exister et ne pas √™tre vide
    - au moins 1 exp√©rience
    - au moins 1 formation
    - comp√©tences non vides (techniques OU fonctionnelles)

    Args:
        data: Dict JSON extrait

    Returns:
        True si extraction valide, False sinon
    """
    try:
        # V√©rifier nom
        contact = data.get("contact", {})
        if not isinstance(contact, dict):
            logger.warning("Contact n'est pas un dict")
            return False

        nom = contact.get("nom", "").strip() if contact.get("nom") else None
        if not nom:
            logger.warning("‚ùå VALIDATION: Nom absent ou vide")
            return False

        # V√©rifier exp√©riences
        experiences = data.get("experiences", [])
        if not experiences or (isinstance(experiences, list) and len(experiences) == 0):
            logger.warning("‚ùå VALIDATION: Aucune exp√©rience trouv√©e")
            return False

        # V√©rifier formations
        formations = data.get("formations", [])
        if not formations or (isinstance(formations, list) and len(formations) == 0):
            logger.warning("‚ùå VALIDATION: Aucune formation trouv√©e")
            return False

        # V√©rifier comp√©tences
        competences = data.get("competences", {})
        if isinstance(competences, dict):
            tech = competences.get("techniques", [])
            fonc = competences.get("fonctionnelles", [])
            if not tech and not fonc:
                logger.warning("‚ùå VALIDATION: Aucune comp√©tence trouv√©e")
                return False
        elif isinstance(competences, list) and len(competences) == 0:
            logger.warning("‚ùå VALIDATION: Aucune comp√©tence trouv√©e (list)")
            return False

        logger.info(f"‚úì VALIDATION R√âUSSIE: {nom} | "
                   f"Exp:{len(experiences)} | Form:{len(formations)} | Comp√©tences OK")
        return True

    except Exception as e:
        logger.error(f"Erreur lors de la validation: {e}")
        return False


# =============================================================================
# EXTRACTION BAS√âE SUR SPACY (Fallback pour CV mal structur√©s)
# =============================================================================

def model_based_extraction(text: str) -> Dict[str, Any]:
    """
    Extrait les informations du CV en utilisant le mod√®le spaCy entra√Æn√©.

    Utilise les entit√©s reconnues (PERSON_NAME, COMPANY, SCHOOL, DIPLOMA, SKILL, LANGUAGE, etc.)
    pour reconstruire un JSON structur√© compatible avec le format existant.

    Args:
        text: Texte brut du CV

    Returns:
        Dict structur√© avec contact, comp√©tences, formations, exp√©riences, langues, loisirs
    """
    logger.info("üöÄ Extraction bas√©e sur le mod√®le spaCy (ML-based)...")

    nlp = load_spacy_model()

    # Traiter le texte avec spaCy
    doc = nlp(text)

    # Initialiser les conteneurs
    extracted = {
        "contact": {
            "nom": None,
            "email": None,
            "telephone": None,
            "adresse": None,
            "titre_profil": None
        },
        "competences": {
            "techniques": [],
            "fonctionnelles": []
        },
        "formations": [],
        "experiences": [],
        "langues": [],
        "loisirs": [],
        "texte_brut": text
    }

    # Extraction des entit√©s nomm√©es
    person_names = []
    companies = []
    schools = []
    diplomas = []
    job_titles = []
    skills = []
    languages = []
    locations = []
    date_ranges = []

    for ent in doc.ents:
        label = ent.label_
        text_clean = ent.text.strip()

        if label == "PERSON_NAME" and text_clean:
            person_names.append(text_clean)
        elif label == "COMPANY" and text_clean:
            companies.append(text_clean)
        elif label == "SCHOOL" and text_clean:
            schools.append(text_clean)
        elif label == "DIPLOMA" and text_clean:
            diplomas.append(text_clean)
        elif label == "JOB_TITLE" and text_clean:
            job_titles.append(text_clean)
        elif label == "SKILL" and text_clean:
            skills.append(text_clean)
        elif label == "LANGUAGE" and text_clean:
            languages.append(text_clean)
        elif label == "LOCATION" and text_clean:
            locations.append(text_clean)
        elif label == "DATE_RANGE" and text_clean:
            date_ranges.append(text_clean)

    # Construire contact
    if person_names:
        extracted["contact"]["nom"] = person_names[0]  # Premier nom trouv√©

    if locations:
        extracted["contact"]["adresse"] = locations[0]

    # Chercher email et t√©l√©phone par regex (spaCy ne les identifie pas toujours)
    email_match = re.search(r"[\w\.-]+@[\w\.-]+\.\w+", text)
    if email_match:
        extracted["contact"]["email"] = email_match.group(0)

    tel_match = re.search(r"(?:0|\+33)[\s\d\(\)\.]{8,20}", text)
    if tel_match:
        extracted["contact"]["telephone"] = tel_match.group(0)

    # Titre profil : utiliser le premier job_title si disponible
    if job_titles:
        extracted["contact"]["titre_profil"] = job_titles[0]

    # Comp√©tences : tout dans "techniques" si pas de distinction claire
    # On essaie de classifier par pattern
    tech_keywords = {
        'python', 'java', 'javascript', 'react', 'angular', 'node', 'docker',
        'sql', 'postgres', 'mongodb', 'git', 'aws', 'azure', 'kubernetes',
        'api', 'rest', 'graphql', 'devops', 'ci/cd', 'linux', 'windows'
    }

    for skill in skills:
        skill_lower = skill.lower()
        if any(tech in skill_lower for tech in tech_keywords):
            extracted["competences"]["techniques"].append(skill)
        else:
            extracted["competences"]["fonctionnelles"].append(skill)

    # Si pas de distinction technique/fonctionnelle, tout en techniques
    if not extracted["competences"]["techniques"] and not extracted["competences"]["fonctionnelles"]:
        extracted["competences"]["techniques"] = skills

    # Formations : combiner √©coles + dipl√¥mes
    formations_set = set()
    for school in schools:
        formations_set.add(school)
    for diploma in diplomas:
        formations_set.add(diploma)
    extracted["formations"] = list(formations_set)

    # Exp√©riences : combiner entreprises + job_titles + dates
    experiences_list = []

    # Si on a entreprises et dates, cr√©er des exp√©riences
    for i, company in enumerate(companies):
        date_str = date_ranges[i] if i < len(date_ranges) else ""
        job_str = job_titles[i] if i < len(job_titles) else ""

        exp_text = f"{company}"
        if job_str:
            exp_text = f"{job_str} chez {exp_text}"
        if date_str:
            exp_text = f"{exp_text} ({date_str})"

        experiences_list.append(exp_text.strip())

    # Si pas assez d'exp√©riences, chercher des patterns dans le texte
    if not experiences_list:
        # Chercher des lignes qui commencent par une date ou contiennent des mots-cl√©s
        lines = text.split('\n')
        for line in lines:
            if re.search(r'\b(20|19)\d{2}\b', line) and len(line) > 10:
                experiences_list.append(line.strip())

    extracted["experiences"] = experiences_list if experiences_list else ["Exp√©riences √† extraire manuellement"]

    # Langues
    extracted["langues"] = languages if languages else []

    logger.info(f"‚úì Extraction ML compl√®te: {extracted['contact']['nom']}")

    return extracted


# =============================================================================
# FUSION INTELLIGENTE DES R√âSULTATS
# =============================================================================

def merge_extractions(rules_based: Dict, ml_based: Dict) -> Dict:
    """
    Fusionne intelligemment l'extraction par r√®gles et celle par ML.
    Prend le meilleur de chaque approche.

    Strat√©gie :
    - Contact : ML si plus complet, sinon r√®gles
    - Comp√©tences : fusion (union)
    - Formations : ML si plus nombreuses, sinon r√®gles
    - Exp√©riences : r√®gles (plus structur√©es), ML pour combler les gaps
    - Langues : fusion
    """
    merged = rules_based.copy()

    # Contact : prendre les champs vides de ML
    for key in ["email", "telephone", "adresse", "titre_profil"]:
        if not merged.get("contact", {}).get(key) and ml_based.get("contact", {}).get(key):
            merged["contact"][key] = ml_based["contact"][key]

    # Comp√©tences : fusion (union) si les deux ont des r√©sultats
    if ml_based.get("competences"):
        merged_comp = merged.get("competences", {})
        ml_comp = ml_based.get("competences", {})

        if isinstance(merged_comp, dict) and isinstance(ml_comp, dict):
            tech = set(merged_comp.get("techniques", []))
            tech.update(ml_comp.get("techniques", []))

            fonc = set(merged_comp.get("fonctionnelles", []))
            fonc.update(ml_comp.get("fonctionnelles", []))

            merged["competences"] = {
                "techniques": list(tech),
                "fonctionnelles": list(fonc)
            }

    # Formations : garder les deux listes (meilleure couverture)
    if ml_based.get("formations"):
        merged_forms = set(merged.get("formations", []))
        merged_forms.update(ml_based.get("formations", []))
        merged["formations"] = list(merged_forms)

    # Exp√©riences : garder les r√®gles (plus fiables), ML pour combler les gaps
    if len(merged.get("experiences", [])) < 2 and ml_based.get("experiences"):
        merged["experiences"].extend(ml_based["experiences"])

    # Langues : fusion
    if ml_based.get("langues"):
        merged_langs = set(merged.get("langues", []))
        merged_langs.update(ml_based.get("langues", []))
        merged["langues"] = list(merged_langs)

    return merged


# =============================================================================
# FONCTION PRINCIPALE : EXTRACTION HYBRIDE
# =============================================================================

def extract_cv_hybrid(file_path: str, extract_robust_fn, extract_text_fn) -> Dict[str, Any]:
    """
    Pipeline hybride intelligent :
    1. Essayer extraction par r√®gles (robust_extractor)
    2. Valider le r√©sultat
    3. Si invalide ‚Üí extraction bas√©e sur spaCy
    4. Retourner le meilleur r√©sultat

    Args:
        file_path: Chemin du fichier CV (PDF/DOCX)
        extract_robust_fn: Fonction d'extraction par r√®gles (robust_extractor.extract_cv_robust)
        extract_text_fn: Fonction d'extraction de texte brut

    Returns:
        Dict JSON structur√© avec contact, comp√©tences, formations, exp√©riences, etc.
    """
    logger.info(f"\n{'='*70}")
    logger.info(f"EXTRACTION HYBRIDE: {Path(file_path).name}")
    logger.info(f"{'='*70}")

    # PHASE 1: Extraction par r√®gles
    logger.info("\n[PHASE 1] Extraction par R√àGLES (robust_extractor)...")
    try:
        rules_result = extract_robust_fn(file_path)
        logger.info("‚úì Extraction par r√®gles compl√©t√©e")
    except Exception as e:
        logger.error(f"Erreur extraction r√®gles: {e}")
        rules_result = None

    # PHASE 2: Validation
    if rules_result:
        is_valid = is_valid_extraction(rules_result)

        if is_valid:
            logger.info("\n‚úÖ R√âSULTAT VALIDE - Utilisation extraction par r√®gles")
            logger.info(f"{'='*70}\n")
            return rules_result

        logger.warning("\n‚ö†Ô∏è R√âSULTAT INVALIDE - Recours au mod√®le spaCy...")
    else:
        logger.warning("\n‚ö†Ô∏è Extraction par r√®gles √©chou√©e - Recours au mod√®le spaCy...")

    # PHASE 3: Extraction par ML (fallback)
    logger.info("\n[PHASE 2] Extraction par MOD√àLE spaCy (ML)...")
    try:
        text = extract_text_fn(file_path)
        ml_result = model_based_extraction(text)
        logger.info("‚úì Extraction par ML compl√©t√©e")
    except Exception as e:
        logger.error(f"Erreur extraction ML: {e}")
        # Retourner l'extraction par r√®gles m√™me si invalide
        if rules_result:
            logger.info("Retour au r√©sultat par r√®gles (m√™me si invalide)")
            return rules_result
        # Sinon retourner une extraction vide
        return {
            "contact": {"nom": "Inconnu", "email": None, "telephone": None, "adresse": None, "titre_profil": None},
            "competences": {"techniques": [], "fonctionnelles": []},
            "formations": [],
            "experiences": [],
            "langues": [],
            "loisirs": [],
            "texte_brut": ""
        }

    # PHASE 4: Fusion intelligente
    if rules_result:
        logger.info("\n[PHASE 3] Fusion intelligente des r√©sultats...")
        final_result = merge_extractions(rules_result, ml_result)
        logger.info("‚úì R√©sultats fusionn√©s")
    else:
        final_result = ml_result
        logger.info("‚úì Utilisation r√©sultat ML uniquement")

    # PHASE 5: Validation finale
    if is_valid_extraction(final_result):
        logger.info(f"\n‚úÖ R√âSULTAT FINAL VALIDE - Pipeline hybride r√©ussi")
    else:
        logger.warning(f"\n‚ö†Ô∏è R√©sultat final suboptimal, mais utilisable")

    logger.info(f"{'='*70}\n")
    return final_result

