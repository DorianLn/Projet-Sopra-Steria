"""
G√©n√©rateur de donn√©es d'entra√Ænement √† partir des CV existants.

Ce script:
1. Lit les fichiers JSON de sortie existants
2. G√©n√®re des exemples d'entra√Ænement NER annot√©s
3. Enrichit les donn√©es d'entra√Ænement automatiquement
"""

from pathlib import Path
from typing import List, Tuple, Dict, Any

import os
import json
import re

try:
    import spacy
    from spacy.training.iob_utils import offsets_to_biluo_tags
    _HAS_SPACY = True
except Exception:
    _HAS_SPACY = False

# Chemin vers les CV analys√©s
OUTPUT_DIR = Path(__file__).parent.parent / "data" / "output"


def normalize_text(s: str) -> str:
    """Normalize le texte pour la g√©n√©ration d'exemples.

    - remplace les retours chariot par des espaces
    - collapse les espaces multiples
    - supprime les caract√®res de contr√¥le invisibles
    """
    if not isinstance(s, str):
        return s
    # remplacer retours chariot et tab par espace
    t = s.replace('\r', ' ').replace('\n', ' ')
    t = t.replace('\t', ' ')
    # retirer caract√®res invisibles courants
    t = re.sub(r'[\u200B-\u200D\uFEFF]', '', t)
    # collapse espaces multiples
    t = re.sub(r'\s+', ' ', t).strip()
    return t


def load_existing_results() -> List[Dict[str, Any]]:
    """Charge tous les r√©sultats JSON existants."""
    results = []
    
    if not OUTPUT_DIR.exists():
        print(f"Dossier {OUTPUT_DIR} non trouv√©")
        return results
    
    for json_file in OUTPUT_DIR.glob("*.json"):
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                data = json.load(f)
                data["_source_file"] = json_file.name
                results.append(data)
        except Exception as e:
            print(f"Erreur lecture {json_file}: {e}")
    
    return results


def generate_ner_example_from_header(cv_data: Dict) -> List[Tuple[str, Dict]]:
    """G√©n√®re des exemples NER depuis les donn√©es d'en-t√™te."""
    examples = []
    
    contact = cv_data.get("contact", {})
    nom = contact.get("nom")
    email = contact.get("email")
    telephone = contact.get("telephone")
    adresse = contact.get("adresse")
    
    if not nom:
        return examples
    
    # Construire un en-t√™te synth√©tique
    header_parts = [nom]
    if telephone:
        header_parts.append(telephone)
    if email:
        header_parts.append(email)
    if adresse:
        header_parts.append(adresse)
    
    header_text = "\n".join(header_parts)
    header_text = normalize_text(header_text)

    # Annoter
    entities = []
    
    # Position du nom
    nom_start = header_text.find(nom)
    if nom_start >= 0:
        entities.append((nom_start, nom_start + len(nom), "PERSON_NAME"))
    
    # Position de l'adresse
    if adresse and adresse in header_text:
        addr_start = header_text.find(adresse)
        entities.append((addr_start, addr_start + len(adresse), "LOCATION"))
    
    if entities:
        examples.append((header_text, {"entities": entities}))
    
    return examples


def generate_ner_example_from_formation(formation: Dict) -> List[Tuple[str, Dict]]:
    """G√©n√®re des exemples NER depuis une formation.

    Accepte soit un dict structur√© (avec 'etablissement', 'diplome', 'dates'),
    soit une string brute. Utilise des heuristiques pour extraire des entit√©s
    lorsque l'entr√©e est une string.
    """
    examples = []

    # Si c'est une string, essayer d'en extraire des informations simples
    if isinstance(formation, str):
        text = normalize_text(formation.strip())
        if not text:
            return examples

        entities = []
        # Chercher une plage d'ann√©es (ex: 2018-2020 ou 2018 ‚Äì 2020)
        date_match = re.search(r"(19|20)\d{2}(?:\s*[-‚Äì√†‚Äî]\s*(19|20)\d{2})?", text)
        if date_match:
            s, e = date_match.span()
            entities.append((s, e, "DATE_RANGE"))

        # Chercher un dipl√¥me par mots-cl√©s
        dip_match = re.search(r"\b(master|licence|bachelor|bts|dipl[o√¥]me|ing[e√©]nieur|doctorat|mba|certificat)\b", text, re.I)
        if dip_match:
            s, e = dip_match.span()
            # √©tendre le span pour prendre la phrase autour du dipl√¥me si possible
            rest_end = text.find(",", e)
            if rest_end == -1:
                rest_end = len(text)
            entities.append((s, rest_end, "DIPLOMA"))

        # Chercher un √©tablissement commun
        school_match = re.search(r"\b(Universit[e√©]|√âcole|EPITA|HEC|Polytechnique|IMT|Lyc[e√©e]|INSA|ENS|IUT|Universit√©)\b", text, re.I)
        if school_match:
            s, e = school_match.span()
            rest_end = text.find("(", e)
            if rest_end == -1:
                rest_end = len(text)
            entities.append((s, rest_end, "SCHOOL"))

        if entities:
            examples.append((text, {"entities": entities}))
        return examples

    # Si c'est d√©j√† un dict structur√©
    etablissement = formation.get("etablissement", "") if isinstance(formation, dict) else ""
    diplome = formation.get("diplome", "") if isinstance(formation, dict) else ""
    dates = formation.get("dates", "") if isinstance(formation, dict) else ""

    if not (etablissement or diplome or dates):
        return examples

    # Diff√©rents formats
    formats = []

    if dates and diplome and etablissement:
        formats.append(f"{dates}: {diplome} - {etablissement}")
        formats.append(f"{diplome} | {etablissement} | {dates}")
        formats.append(f"{etablissement} ({dates})\n{diplome}")
    elif diplome and etablissement:
        formats.append(f"{diplome} - {etablissement}")
    elif etablissement:
        formats.append(f"{etablissement}")
    elif diplome:
        formats.append(f"{diplome}")

    for text in formats:
        text = normalize_text(text)
        entities = []

        if dates and dates in text:
            pos = text.find(dates)
            entities.append((pos, pos + len(dates), "DATE_RANGE"))

        if diplome and diplome in text:
            pos = text.find(diplome)
            entities.append((pos, pos + len(diplome), "DIPLOMA"))

        if etablissement and etablissement in text:
            pos = text.find(etablissement)
            entities.append((pos, pos + len(etablissement), "SCHOOL"))

        if entities:
            examples.append((text, {"entities": entities}))

    return examples


def generate_ner_example_from_experience(experience: Dict) -> List[Tuple[str, Dict]]:
    """G√©n√®re des exemples NER depuis une exp√©rience.

    Accepte soit un dict structur√© (avec 'entreprise','poste','dates') soit une string.
    Utilise heuristiques pour extraire dates, poste et entreprise lorsque l'entr√©e est une string.
    """
    examples = []

    if isinstance(experience, str):
        text = normalize_text(experience.strip())
        if not text:
            return examples

        entities = []
        # Dates
        date_match = re.search(r"(19|20)\d{2}(?:\s*[-‚Äì√†‚Äî]\s*(19|20)\d{2})?|(?:Janvier|F[e√©]vrier|Mars|Avril|Mai|Juin|Juillet|Ao[u√ª]t|Septembre|Octobre|Novembre|D[e√©]cembre)\s+\d{4}", text, re.I)
        if date_match:
            s, e = date_match.span()
            entities.append((s, e, "DATE_RANGE"))

        # Poste (heuristique: mots m√©tiers)
        poste_match = re.search(r"\b(D[e√©]veloppeur|Developpeur|Ing[e√©]nieur|Consultant|Chef de Projet|Lead|Architecte|Stagiaire|Product Owner|Analyst|Business Analyst)\b", text, re.I)
        if poste_match:
            s, e = poste_match.span()
            # √©tendre jusqu'√† 'chez' ou fin
            chez_idx = text.lower().find("chez ", e)
            end_pos = chez_idx if chez_idx != -1 else min(len(text), e + 40)
            entities.append((s, end_pos, "JOB_TITLE"))

        # Entreprise (heuristique: 'chez X' ou mot apr√®s '- ' ou ', ')
        chez = re.search(r"chez\s+([A-Za-z√Ä-√ñ√ò-√∂√∏-√ø0-9 &.-]{2,})", text, re.I)
        if chez:
            s = chez.start(1)
            e = chez.end(1)
            entities.append((s, e, "COMPANY"))
        else:
            parts = re.split(r"[-‚Äì,:]\s*", text)
            if len(parts) >= 2:
                candidate = parts[-1].strip()
                if 1 < len(candidate) <= 60 and any(c.isalpha() for c in candidate):
                    idx = text.rfind(candidate)
                    if idx != -1:
                        entities.append((idx, idx + len(candidate), "COMPANY"))

        if entities:
            examples.append((text, {"entities": entities}))
        return examples

    # Cas dict structur√©
    entreprise = experience.get("entreprise", "") if isinstance(experience, dict) else ""
    poste = experience.get("poste", "") if isinstance(experience, dict) else ""
    dates = experience.get("dates", "") if isinstance(experience, dict) else ""

    if not (entreprise or poste or dates):
        return examples

    formats = []

    if dates and poste and entreprise:
        formats.append(f"{dates}: {poste} chez {entreprise}")
        formats.append(f"{poste} | {entreprise} | {dates}")
        formats.append(f"{entreprise} - {poste} ({dates})")
    elif poste and entreprise:
        formats.append(f"{poste} - {entreprise}")
    elif entreprise:
        formats.append(f"{entreprise}")
    elif poste:
        formats.append(f"{poste}")

    for text in formats:
        text = normalize_text(text)
        entities = []

        if dates and dates in text:
            pos = text.find(dates)
            entities.append((pos, pos + len(dates), "DATE_RANGE"))

        if poste and poste in text:
            pos = text.find(poste)
            entities.append((pos, pos + len(poste), "JOB_TITLE"))

        if entreprise and entreprise in text:
            pos = text.find(entreprise)
            entities.append((pos, pos + len(entreprise), "COMPANY"))

        if entities:
            examples.append((text, {"entities": entities}))

    return examples


def generate_training_data_from_results(results: List[Dict]) -> List[Tuple[str, Dict]]:
    """G√©n√®re toutes les donn√©es d'entra√Ænement."""
    all_examples = []
    
    for cv_data in results:
        # En-t√™te
        all_examples.extend(generate_ner_example_from_header(cv_data))
        
        # Formations
        for formation in cv_data.get("formations", []):
            all_examples.extend(generate_ner_example_from_formation(formation))
        
        # Exp√©riences
        for experience in cv_data.get("experiences", []):
            all_examples.extend(generate_ner_example_from_experience(experience))
    
    return all_examples


def validate_examples(examples: List[Tuple[str, Dict]]) -> List[Tuple[str, Dict]]:
    """Valide les exemples g√©n√©r√©s.

    Filtre aussi les exemples dont les offsets ne s'alignent pas avec la tokenisation
    (utilise spaCy si disponible)."""
    valid = []
    rejected = []

    nlp = None
    if _HAS_SPACY:
        try:
            nlp = spacy.blank("fr")
        except Exception:
            nlp = None

    for text, annotations in examples:
        entities = annotations.get("entities", [])
        all_valid = True
        # v√©rification basique des bornes
        for start, end, label in entities:
            if start < 0 or end > len(text) or start >= end:
                all_valid = False
                break
            # V√©rifier que l'entit√© n'est pas vide
            entity_text = text[start:end].strip()
            if not entity_text:
                all_valid = False
                break
        
        # Si spaCy est disponible, v√©rifier l'alignement sur la tokenisation
        if all_valid and nlp is not None and entities:
            try:
                doc = nlp.make_doc(text)
                # Tentative de r√©alignement automatique faible: pour chaque entit√©,
                # essayer d'obtenir un char_span align√©; si pas possible, chercher
                # le texte de l'entit√© dans le document et recalculer les offsets.
                realigned = []
                for (start, end, label) in entities:
                    span = doc.char_span(start, end, label=label, alignment_mode='expand')
                    if span is not None:
                        realigned.append((span.start_char, span.end_char, label))
                    else:
                        # tentative par recherche textuelle du texte de l'entit√©
                        ent_text = text[start:end].strip()
                        if ent_text:
                            idx = text.find(ent_text)
                            if idx != -1:
                                realigned.append((idx, idx + len(ent_text), label))
                            else:
                                realigned.append((start, end, label))
                        else:
                            realigned.append((start, end, label))

                # v√©rifier avec offsets_to_biluo_tags si les realigned passent
                try:
                    biluo = offsets_to_biluo_tags(doc, realigned)
                    if any(tag == '-' for tag in biluo):
                        all_valid = False
                    else:
                        # remplacer entities par realigned
                        annotations['entities'] = realigned
                except Exception:
                    all_valid = False
            except Exception:
                # En cas d'erreur, ne pas casser la g√©n√©ration ‚Äî rejeter l'exemple
                all_valid = False

        if all_valid and entities:
            valid.append((text, annotations))
        else:
            rejected.append((text, annotations))

    # Petit rapport pour debug
    if rejected:
        print(f"\n[generate_training_data] {len(rejected)} exemples rejet√©s (mauvais offsets / alignement)")
        # afficher quelques exemples rejet√©s (jusqu'√† 5)
        for text, ann in rejected[:5]:
            print(f" - Exemple rejet√©: {text[:60]!s}... -> {ann.get('entities', [])}")

    return valid


def export_to_python(examples: List[Tuple[str, Dict]], output_file: str):
    """Exporte les exemples en format Python."""
    with open(output_file, "w", encoding="utf-8") as f:
        f.write("# Donn√©es d'entra√Ænement g√©n√©r√©es automatiquement\n\n")
        f.write("GENERATED_NER_DATA = [\n")
        
        for text, annotations in examples:
            # √âchapper les caract√®res
            text_escaped = text.replace("\\", "\\\\").replace('"', '\\"').replace("\n", "\\n")
            
            f.write(f'    (\n')
            f.write(f'        "{text_escaped}",\n')
            f.write(f'        {{"entities": {annotations.get("entities", [])}}}\n')
            f.write(f'    ),\n')
        
        f.write("]\n")


def main():
    print("=" * 60)
    print("G√âN√âRATION DE DONN√âES D'ENTRA√éNEMENT")
    print("=" * 60)
    
    # Charger les r√©sultats existants
    print("\nüìÇ Chargement des CV existants...")
    results = load_existing_results()
    print(f"   {len(results)} fichiers JSON charg√©s")
    
    if not results:
        print("\n‚ö†Ô∏è Aucun r√©sultat trouv√©. Cr√©ez d'abord des CV analys√©s.")
        return
    
    # G√©n√©rer les exemples
    print("\nüîÑ G√©n√©ration des exemples...")
    examples = generate_training_data_from_results(results)
    print(f"   {len(examples)} exemples g√©n√©r√©s")
    
    # Valider
    print("\n‚úì Validation...")
    valid_examples = validate_examples(examples)
    print(f"   {len(valid_examples)} exemples valides")
    
    # Exporter
    output_file = Path(__file__).parent / "generated_data.py"
    export_to_python(valid_examples, str(output_file))
    print(f"\nüíæ Export√© vers: {output_file}")
    
    # Afficher quelques exemples
    print("\nüìù Exemples g√©n√©r√©s:")
    for text, ann in valid_examples[:5]:
        print(f"\n   Texte: {text[:60]}...")
        for start, end, label in ann.get("entities", []):
            print(f"      [{label}] '{text[start:end]}'")


if __name__ == "__main__":
    main()
