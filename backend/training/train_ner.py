"""
Script d'entra√Ænement du NER spaCy personnalis√© pour l'extraction de CV.

Ce script:
1. Charge le mod√®le fr_core_news_md existant
2. Ajoute des labels NER personnalis√©s (PERSON_NAME, COMPANY, SCHOOL, etc.)
3. Entra√Æne le NER avec les donn√©es annot√©es
4. Sauvegarde le mod√®le entra√Æn√©

Usage:
    python train_ner.py [--iterations 30] [--output models/cv_ner]
"""

import os
import sys
import random
import json
from pathlib import Path
from datetime import datetime

import spacy
from spacy.training import Example
from spacy.util import minibatch, compounding

# Ajouter le dossier parent au path
sys.path.insert(0, str(Path(__file__).parent.parent))

from training.training_data import NER_TRAINING_DATA, get_ner_labels, validate_training_data
from training.generated_data import GENERATED_NER_DATA

TRAIN_DATA = NER_TRAINING_DATA + GENERATED_NER_DATA

def remove_overlapping_entities(examples):
    cleaned = []

    for text, ann in examples:
        ents = ann["entities"]

        # Trier par longueur d√©croissante (priorit√© aux spans les plus longs)
        ents = sorted(ents, key=lambda x: (x[1] - x[0]), reverse=True)

        kept = []
        occupied = set()

        for start, end, label in ents:
            overlap = False
            for i in range(start, end):
                if i in occupied:
                    overlap = True
                    break

            if not overlap:
                kept.append((start, end, label))
                for i in range(start, end):
                    occupied.add(i)

        if kept:
            cleaned.append((text, {"entities": kept}))

    return cleaned

SKILL_TECH_SET = {
    "python", "java", "javascript", "react", "node", "docker", "kubernetes",
    "sql", "aws", "azure", "git", "linux", "spark", "hadoop", "tensorflow", "pandas"
}

SKILL_FUNC_SET = {
    "scrum", "agile", "kanban", "gestion de projet", "product management",
    "conduite du changement", "analyse m√©tier", "pilotage", "recueil du besoin"
}

def normalize_skill_labels(examples):
    normalized = []

    for text, ann in examples:
        new_ents = []

        for start, end, label in ann["entities"]:
            span = text[start:end].lower()

            if label == "SKILL":
                if any(skill in span for skill in SKILL_TECH_SET):
                    new_ents.append((start, end, "SKILL_TECH"))
                elif any(skill in span for skill in SKILL_FUNC_SET):
                    new_ents.append((start, end, "SKILL_FUNC"))
                else:
                    continue  # on supprime le SKILL non classifiable
            else:
                new_ents.append((start, end, label))

        if new_ents:
            normalized.append((text, {"entities": new_ents}))

    return normalized


def create_training_examples(nlp, training_data):
    """Convertit les donn√©es d'entra√Ænement en objets Example spaCy."""
    examples = []
    for text, annotations in training_data:
        doc = nlp.make_doc(text)
        example = Example.from_dict(doc, annotations)
        examples.append(example)
    return examples


def train_ner(
    base_model: str = "fr_core_news_md",
    output_dir: str = None,
    n_iter: int = 30,
    dropout: float = 0.35
):
    """
    Entra√Æne le NER avec les donn√©es annot√©es.
    
    Args:
        base_model: Mod√®le spaCy de base √† utiliser
        output_dir: Dossier de sortie pour le mod√®le entra√Æn√©
        n_iter: Nombre d'it√©rations d'entra√Ænement
        dropout: Taux de dropout
    
    Returns:
        Le mod√®le entra√Æn√©
    """
    # Validation des donn√©es
    print("üîç Validation des donn√©es d'entra√Ænement...")
    errors = validate_training_data(TRAIN_DATA)
    if errors:
        print(f"‚ùå {len(errors)} erreurs dans les donn√©es:")
        for e in errors[:5]:
            print(f"   {e}")
        raise ValueError("Donn√©es d'entra√Ænement invalides")
    print(f"‚úì {len(TRAIN_DATA)} exemples valides")
    
    # Charger le mod√®le de base
    print(f"\nüì¶ Chargement du mod√®le de base '{base_model}'...")
    try:
        nlp = spacy.load(base_model)
    except OSError:
        print(f"Mod√®le '{base_model}' non trouv√©. T√©l√©chargement...")
        os.system(f"python -m spacy download {base_model}")
        nlp = spacy.load(base_model)
    
    # R√©cup√©rer ou cr√©er le composant NER
    if "ner" not in nlp.pipe_names:
        ner = nlp.add_pipe("ner", last=True)
    else:
        ner = nlp.get_pipe("ner")
    
    # Ajouter les nouveaux labels
    print("\nüè∑Ô∏è Ajout des labels personnalis√©s...")
    labels = set(get_ner_labels())

    for _, ann in GENERATED_NER_DATA:
        for _, _, label in ann["entities"]:
            labels.add(label)

    labels = sorted(labels)

    for label in labels:
        ner.add_label(label)
        print(f"   + {label}")

    
    # Pr√©parer les exemples d'entra√Ænement
    print("\nüìö Pr√©paration des exemples d'entra√Ænement...")
    shuffled_data = TRAIN_DATA.copy()
    random.shuffle(shuffled_data)

    normalized_data = normalize_skill_labels(shuffled_data)
    filtered_data = remove_overlapping_entities(normalized_data)

    examples = create_training_examples(nlp, filtered_data)
    
    # Obtenir les autres composants du pipeline √† d√©sactiver pendant l'entra√Ænement
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != "ner"]
    
    # Entra√Ænement
    print(f"\nüöÄ D√©but de l'entra√Ænement ({n_iter} it√©rations)...")
    print("-" * 50)
    
    with nlp.disable_pipes(*other_pipes):
        # Initialiser le NER avec les exemples
        nlp.initialize(lambda: examples)
        
        for iteration in range(n_iter):
            random.shuffle(examples)
            losses = {}
            
            # Cr√©er des mini-batches
            batches = minibatch(examples, size=compounding(4.0, 32.0, 1.001))
            
            for batch in batches:
                nlp.update(
                    batch,
                    drop=dropout,
                    losses=losses
                )
            
            if (iteration + 1) % 5 == 0 or iteration == 0:
                print(f"   It√©ration {iteration + 1:3d}/{n_iter}: loss = {losses.get('ner', 0):.4f}")
    
    print("-" * 50)
    print("‚úì Entra√Ænement termin√©!")
    
    # Sauvegarder le mod√®le
    if output_dir:
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        nlp.to_disk(output_path)
        print(f"\nüíæ Mod√®le sauvegard√© dans: {output_path}")
        
        # Sauvegarder les m√©tadonn√©es
        meta = {
            "base_model": base_model,
            "trained_on": datetime.now().isoformat(),
            "iterations": n_iter,
            "labels": labels,
            "examples_count": len(filtered_data)
        }
        with open(output_path / "training_meta.json", "w", encoding="utf-8") as f:
            json.dump(meta, f, indent=2, ensure_ascii=False)
    
    return nlp


def test_model(nlp, test_texts=None):
    """Teste le mod√®le sur des textes exemples."""
    if test_texts is None:
        test_texts = [
            "Marie DUPONT\nD√©veloppeuse Python Senior\n06 12 34 56 78",
            "2020-2023: Master Informatique - Universit√© Paris-Saclay",
            "Lead DevOps chez Amazon Web Services depuis 2021",
            "Comp√©tences: Python, Java, Docker, Kubernetes, AWS",
            "Langues: Fran√ßais (natif), Anglais (courant)"
        ]
    
    print("\nüß™ Test du mod√®le:")
    print("=" * 60)
    
    for text in test_texts:
        doc = nlp(text)
        print(f"\nTexte: {text[:60]}...")
        if doc.ents:
            for ent in doc.ents:
                print(f"   [{ent.label_:15}] '{ent.text}'")
        else:
            print("   (aucune entit√© d√©tect√©e)")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Entra√Ænement du NER spaCy pour CV")
    parser.add_argument("--iterations", "-n", type=int, default=30,
                        help="Nombre d'it√©rations (d√©faut: 30)")
    parser.add_argument("--output", "-o", type=str, default="models/cv_ner",
                        help="Dossier de sortie (d√©faut: models/cv_ner)")
    parser.add_argument("--base-model", "-m", type=str, default="fr_core_news_md",
                        help="Mod√®le de base (d√©faut: fr_core_news_md)")
    parser.add_argument("--test", "-t", action="store_true",
                        help="Tester le mod√®le apr√®s entra√Ænement")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("   ENTRA√éNEMENT NER PERSONNALIS√â POUR CV")
    print("=" * 60)
    
    # Chemin absolu pour la sortie
    output_dir = Path(__file__).parent.parent / args.output
    
    # Entra√Ænement
    nlp = train_ner(
        base_model=args.base_model,
        output_dir=str(output_dir),
        n_iter=args.iterations
    )
    
    # Test si demand√©
    if args.test:
        test_model(nlp)
    
    print("\n‚úÖ Termin√©!")


if __name__ == "__main__":
    main()
